{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from site_visit import *\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.spatial import distance_matrix\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "#color_lst = px.colors.qualitative.Light24\n",
    "\n",
    "prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "color_lst = prop_cycle.by_key()['color']\n",
    "\n",
    "color_lst = px.colors.qualitative.Light24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def eval_on_random(config, vocab_lsts, count_lsts):\n",
    "def eval_on_random(config, spatial_data, vocab_lsts, count_lsts):\n",
    "    \n",
    "    #_data = [row for row in csv.DictReader(open(config['DATA']), delimiter='\\t')]\n",
    "    sample = []\n",
    "    p = config['DEV']\n",
    "    for t in range(1,7):\n",
    "        for step in spatial_data[(p,str(t))]:\n",
    "            if step % 450 == 0:\n",
    "                sample.append({'participant': p, 'task': str(t), 'step': step, 'lemma': 'NA', 'pos': 'NA'})\n",
    "                \n",
    "    #sample = random.sample(_data, 10)\n",
    "    #sample = [e for e in _data if e['participant'] == config['DEV']]\n",
    "    #sample = sorted(sample, key=lambda e:(e['task'], int(e['step'])))\n",
    "    \n",
    "    sys.stderr.write(\"Data: %d items\\n\"%len(sample))\n",
    "\n",
    "    X, y, meta = _make_raw_spatial_mats(sample, spatial_data,\n",
    "                                               feat_lst=config['FEATURES'],\n",
    "                                               use_objs=config['OBJS'],\n",
    "                                               use_most_moving=config['MOST_MOVING'],\n",
    "                                               window_size = config['WINDOW'],\n",
    "                                               logdir=None)\n",
    "\n",
    "    return X, y, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(config, logdir):\n",
    "    _data = [row for row in csv.DictReader(open(config['DATA']), delimiter='\\t')]\n",
    "    sys.stderr.write(\"Data: %d items\\n\"%len(_data))\n",
    "\n",
    "    vocab_lsts, count_lsts = get_vocabs(_data, MODE=config['MODE'],\n",
    "                                        K=config['K'], CUTOFF=config['CUTOFF'])\n",
    "    \n",
    "    #top_words = [\"pick_VERB\", \"put_VERB\", \"hold_VERB\", \"eat_VERB\"]\n",
    "    top_words = get_top_words(config['POS'])\n",
    "    #top_words = [w for i, w in enumerate(vocab_lsts[1])\n",
    "    #             if (w.split('_')[1] == config['POS'] and count_lsts[1][i] >= 10)]\n",
    "    #sorted_words = [w for w, c in sorted(zip(vocab_lsts[1], count_lsts[1]), key=lambda e:e[1], reverse=True)]\n",
    "    #sorted_pos = [w for w in sorted_words if (config['POS'] in w) and (w not in top_words)]\n",
    "    #top_words += sorted_pos[:15]\n",
    "    \n",
    "    _data = [d for d in _data if '%s_%s'%(d['lemma'], d['pos']) in top_words]\n",
    "    sys.stderr.write(\"Filtered Data: %d items\\n\"%len(_data))\n",
    "    \n",
    "    X, y, meta = _make_raw_spatial_mats(_data, spatial_data,\n",
    "                                               feat_lst=config['FEATURES'],\n",
    "                                               use_objs=config['OBJS'],\n",
    "                                               use_most_moving=config['MOST_MOVING'],\n",
    "                                               window_size = config['WINDOW'],\n",
    "                                               logdir=logdir)\n",
    "\n",
    "    if config['MODE'] == 'random':\n",
    "        rows, cols = X.shape\n",
    "        X = np.random.rand(rows, cols)\n",
    "    if config['MODE'] == 'oracle':\n",
    "        rows, cols = X.shape\n",
    "        words = sorted(list(set(y)))\n",
    "        X = np.random.rand(rows, len(words))\n",
    "        for i, w in enumerate(y):\n",
    "            X[i, words.index(w)] = 1\n",
    "\n",
    "\n",
    "    return vocab_lsts, count_lsts, top_words, X, y, meta\n",
    "\n",
    "def filter_and_dedup(X, y, meta, top_words):\n",
    "    X_filt, y_filt, m_filt = filter_mats(X, y, meta, top_words)\n",
    "\n",
    "    dedup = []\n",
    "    seen = set()\n",
    "    for i, (mm, yy) in enumerate(zip(m_filt, y_filt)):\n",
    "        part, task, step, steps = mm.split()\n",
    "        if (part, task, yy) not in seen:\n",
    "            seen.add((part, task, yy))\n",
    "            dedup.append(i)\n",
    "\n",
    "    X_filt = X_filt[dedup]\n",
    "    y_filt = y_filt[dedup]\n",
    "    m_filt = m_filt[dedup]\n",
    "    \n",
    "    return X_filt, y_filt, m_filt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_from_config(config):\n",
    "    fnames = ['k', 'win'] \n",
    "    fs = ['K', 'WINDOW'] \n",
    "\n",
    "    return '_'.join(['%s=%s'%(fnames[i],\n",
    "                              config[fs[i]]) for i in range(len(fs))])\n",
    "\n",
    "def make_directories(config):\n",
    "\n",
    "    make = config['LOG']\n",
    "    \n",
    "    # set up directories and such\n",
    "    inp = config['MODE'].split('.')[0].split('/')[-1]\n",
    "\n",
    "    inp += '_' + name_from_config(config)\n",
    "\n",
    "    rootdir = 'spatial_eval/%s/'%inp\n",
    "    \n",
    "    figdir = rootdir + 'red=%s_dim=%s_sup=%s/figures'%(config['RED'],\n",
    "                                                    config['DIM'], config['SUP'])\n",
    "    repdir = rootdir + 'red=%s_dim=%s_sup=%s/reports'%(config['RED'],\n",
    "                                                config['DIM'], config['SUP'])\n",
    "    if make:\n",
    "        if not os.path.exists(figdir):\n",
    "            os.makedirs(figdir)\n",
    "        if not os.path.exists(repdir):\n",
    "            os.makedirs(repdir)\n",
    "        \n",
    "    return repdir, figdir\n",
    "\n",
    "def vocab_mat_figures(outpath, nm, mat, words, mn, mx, fmt='png', save=False):\n",
    "    if not os.path.exists(outpath):\n",
    "        os.makedirs(outpath)\n",
    "\n",
    "    plt.figure(figsize=(4, 0.2*len(words)))\n",
    "    sns.heatmap(mat, cmap=\"coolwarm_r\", vmin=mn, vmax=mx)\n",
    "    plt.yticks(np.arange(len(words))+0.5, words, rotation=\"horizontal\")\n",
    "    if save:\n",
    "        plt.savefig('%s/agg_%s.%s'%(outpath, nm, fmt), bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    \n",
    "def vocab_mat_figures_indv(outpath, tup, mn, mx, fmt='png', save=False):\n",
    "    use_X, use_y, use_m = tup\n",
    "    for w in set(use_y):\n",
    "        idx = [i for i, u in enumerate(use_y) if u == w]\n",
    "        if len(idx) == 0:\n",
    "            continue\n",
    "        plt.figure(figsize=(4, 0.2*len(idx)))\n",
    "        sns.heatmap(use_X[idx, :], cmap=\"coolwarm_r\", vmin=mn, vmax=mx)\n",
    "        plt.title(w)\n",
    "        ticks = use_m[idx]\n",
    "        plt.yticks(np.arange(len(ticks))+0.5, ticks, rotation=\"horizontal\")\n",
    "        if save:\n",
    "            plt.savefig('%s/%s_full.%s'%(outpath, w, fmt), bbox_inches='tight')\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "        \n",
    "def avg_mat(tup, words):\n",
    "    use_X, use_y, use_m = tup\n",
    "    n_obs, n_feat = use_X.shape\n",
    "    mat = np.zeros((len(words), n_feat))\n",
    "    for i, w in enumerate(words):\n",
    "        avg = use_X[use_y == w, :].sum(axis=0)\n",
    "        mat[i, :] = avg\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _max_dist_change_feats(ref, mat):\n",
    "    dists = distance_matrix(ref.reshape(1, -1), mat)\n",
    "\n",
    "    max_idx = [e for e in reversed(dists[0].argsort())][0]\n",
    "    mx = dists[0, max_idx]\n",
    "\n",
    "    mxx, mxy, mxz = mat[max_idx, :]\n",
    "    names = [\"dist\", \"x\", \"y\", \"z\"]\n",
    "    feats = [mx, mxx, mxy, mxz]\n",
    "    return feats, names\n",
    "\n",
    "def _dist_change_feats(s, e):\n",
    "    \n",
    "    d = euclidean(s, e)\n",
    "\n",
    "    names = [\"dist\", \"x\", \"y\", \"z\"]\n",
    "    feats = [d] + [s[i] - e[i] for i in range(3)]\n",
    "    return feats, names\n",
    "\n",
    "def _avg_feats(span, dims):\n",
    "    \n",
    "    avg = span.mean(axis=0)\n",
    "\n",
    "    names = [\"%savg\"%d for d in dims]\n",
    "    feats = [avg[i] for i in range(len(dims))]\n",
    "    return feats, names\n",
    "\n",
    "def _var_feats(span, dims):\n",
    "    \n",
    "    avg = span.std(axis=0)\n",
    "\n",
    "    names = [\"%svar\"%d for d in dims]\n",
    "    feats = [avg[i] for i in range(len(dims))]\n",
    "    return feats, names\n",
    "\n",
    "def _dist_to_obj_feats(obj, ref):\n",
    "\n",
    "    at_start = euclidean(obj[0], ref[0])\n",
    "    at_end = euclidean(obj[-1], ref[-1])\n",
    "    \n",
    "    dists = np.linalg.norm(obj - ref)\n",
    "    avg = dists.mean()\n",
    "    \n",
    "    names = [\"start\", \"end\", \"mean\"]\n",
    "    feats = [at_start, at_end, avg]\n",
    "    return feats, names\n",
    "\n",
    "def get_features(span, all_feats, all_obj_spans, use=None):\n",
    "\n",
    "    feats = []\n",
    "    names = []\n",
    "    \n",
    "    # change in absoluate position, start to max\n",
    "    if (use is None) or ('abs_st_mx' in use):\n",
    "        px = all_feats.index('posX')\n",
    "        py = all_feats.index('posY')\n",
    "        pz = all_feats.index('posZ')\n",
    "        fs, ns = _max_dist_change_feats(span[0, [px, py, pz]], span[:, [px, py, pz]])\n",
    "        if np.isnan(fs).any():\n",
    "            print(\"NaNs!! %s\"%(' '.join(ns)), fs)\n",
    "        names += [\"abs_st_mx\"+e for e in ns]\n",
    "        feats += fs\n",
    "        \n",
    "    # change in absoluate position, end to max\n",
    "    if (use is None) or ('abs_end_mx' in use):\n",
    "        px = all_feats.index('posX')\n",
    "        py = all_feats.index('posY')\n",
    "        pz = all_feats.index('posZ')\n",
    "        fs, ns = _max_dist_change_feats(span[-1, [px, py, pz]], span[:, [px, py, pz]])\n",
    "        if np.isnan(fs).any():\n",
    "            print(\"NaNs!! %s\"%(' '.join(ns)), fs)\n",
    "        names += [\"abs_end_mx\"+e for e in ns]\n",
    "        feats += fs\n",
    "    \n",
    "    # change in absoluate position, start to end\n",
    "    if (use is None) or ('abs_pos_se' in use):\n",
    "        px = all_feats.index('posX')\n",
    "        py = all_feats.index('posY')\n",
    "        pz = all_feats.index('posZ')\n",
    "        fs, ns = _dist_change_feats(span[0, [px, py, pz]], span[-1, [px, py, pz]])\n",
    "        if np.isnan(fs).any():\n",
    "            print(\"NaNs!! %s\"%(' '.join(ns)), fs)\n",
    "        names += [\"abs_pos_se_\"+e for e in ns]\n",
    "        feats += fs\n",
    "        \n",
    "    # distance to the right hand\n",
    "    if (use is None) or ('dist_to_rhand' in use): \n",
    "        px = all_feats.index('posX')\n",
    "        py = all_feats.index('posY')\n",
    "        pz = all_feats.index('posZ')\n",
    "        this = span[:, [px, py, pz]]\n",
    "        ref = all_obj_spans['RightHand'][:, [px, py, pz]]\n",
    "        fs, ns = _dist_to_obj_feats(this, ref)\n",
    "        names += [\"dist_to_rhand_\"+e for e in ns]\n",
    "        feats += fs\n",
    "        \n",
    "    # distance to the left hand\n",
    "    if (use is None) or ('dist_to_lhand' in use):  \n",
    "        px = all_feats.index('posX')\n",
    "        py = all_feats.index('posY')\n",
    "        pz = all_feats.index('posZ')\n",
    "        this = span[:, [px, py, pz]]\n",
    "        ref = all_obj_spans['LeftHand'][:, [px, py, pz]]\n",
    "        fs, ns = _dist_to_obj_feats(this, ref)\n",
    "        names += [\"dist_to_lhand_\"+e for e in ns]\n",
    "        feats += fs\n",
    "    \n",
    "    # change in relative position, start to max\n",
    "    if (use is None) or ('rel_st_mx' in use):\n",
    "        px = all_feats.index('relPosX')\n",
    "        py = all_feats.index('relPosY')\n",
    "        pz = all_feats.index('relPosZ')\n",
    "        fs, ns = _max_dist_change_feats(span[0, [px, py, pz]], span[:, [px, py, pz]])\n",
    "        if np.isnan(fs).any():\n",
    "            print(\"NaNs!! %s\"%(' '.join(ns)), fs)\n",
    "        names += [\"rel_st_mx\"+e for e in ns]\n",
    "        feats += fs\n",
    "        \n",
    "    # change in relative position, start to max\n",
    "    if (use is None) or ('rel_end_mx' in use):\n",
    "        px = all_feats.index('relPosX')\n",
    "        py = all_feats.index('relPosY')\n",
    "        pz = all_feats.index('relPosZ')\n",
    "        fs, ns = _max_dist_change_feats(span[-1, [px, py, pz]], span[:, [px, py, pz]])\n",
    "        if np.isnan(fs).any():\n",
    "            print(\"NaNs!! %s\"%(' '.join(ns)), fs)\n",
    "        names += [\"rel_end_mx\"+e for e in ns]\n",
    "        feats += fs\n",
    "    \n",
    "    # change in relative position, start to end\n",
    "    if (use is None) or ('rel_pos_se' in use):\n",
    "        px = all_feats.index('relPosX')\n",
    "        py = all_feats.index('relPosY')\n",
    "        pz = all_feats.index('relPosZ')\n",
    "        fs, ns = _dist_change_feats(span[0, [px, py, pz]], span[-1, [px, py, pz]])\n",
    "        if np.isnan(fs).any():\n",
    "            print(\"NaNs!! %s\"%(' '.join(ns)), fs)\n",
    "        names += [\"rel_pos_se_\"+e for e in ns]\n",
    "        feats += fs\n",
    "    \n",
    "    # average relative position\n",
    "    if (use is None) or ('rel_pos_avg' in use):\n",
    "        px = all_feats.index('relPosX')\n",
    "        py = all_feats.index('relPosY')\n",
    "        pz = all_feats.index('relPosZ')\n",
    "        fs, ns = _avg_feats(span[:, [px, py, pz]], ['x', 'y', 'z'])\n",
    "        if np.isnan(fs).any():\n",
    "            print(\"NaNs!! avg relpos %s\"%(' '.join(ns)), fs)\n",
    "        names += [\"rel_pos_avg\"+e for e in ns]\n",
    "        feats += fs\n",
    "\n",
    "    # average relative position\n",
    "    if (use is None) or ('rel_pos_var' in use):   \n",
    "        px = all_feats.index('relPosX')\n",
    "        py = all_feats.index('relPosY')\n",
    "        pz = all_feats.index('relPosZ')\n",
    "        fs, ns = _var_feats(span[:, [px, py, pz]], ['x', 'y', 'z'])\n",
    "        if np.isnan(fs).any():\n",
    "            print(\"NaNs!! avg relpos %s\"%(' '.join(ns)), fs)\n",
    "        names += [\"rel_pos_var\"+e for e in ns]\n",
    "        feats += fs\n",
    "    \n",
    "    # average velocity\n",
    "    if (use is None) or ('vel_avg' in use):       \n",
    "        px = all_feats.index('velX')\n",
    "        py = all_feats.index('velY')\n",
    "        pz = all_feats.index('velZ')\n",
    "        fs, ns = _avg_feats(span[:, [px, py, pz]], ['x', 'y', 'z'])\n",
    "        if np.isnan(fs).any():\n",
    "            print(\"NaNs!! avg vel %s\"%(' '.join(ns)), fs)\n",
    "        names += [\"vel_avg\"+e for e in ns]\n",
    "        feats += fs\n",
    "\n",
    "    # average relative velocity\n",
    "    if (use is None) or ('rel_vel_avg' in use):       \n",
    "        px = all_feats.index('relVelX')\n",
    "        py = all_feats.index('relVelY')\n",
    "        pz = all_feats.index('relVelZ')\n",
    "        fs, ns = _avg_feats(span[:, [px, py, pz]], ['x', 'y', 'z'])\n",
    "        if np.isnan(fs).any():\n",
    "            print(\"NaNs!! avg relvel %s\"%(' '.join(ns)), fs)\n",
    "        names += [\"rel_vel_avg\"+e for e in ns]\n",
    "        feats += fs\n",
    "            \n",
    "    return feats, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_moving(obj_spans, fnames):\n",
    "    max_objs = []\n",
    "    x = fnames.index('velX')\n",
    "    y = fnames.index('velY')\n",
    "    z = fnames.index('velZ')\n",
    "    for obj, span in obj_spans.items():\n",
    "        vals, _ = _max_dist_change_feats(span[0, [x, y, z]], span[:, [x, y, z]])\n",
    "        d = vals[0] # euclidean distance\n",
    "        max_objs.append((obj, d))\n",
    "    rank = sorted(max_objs, key=lambda e:e[1], reverse=True)\n",
    "    return [o for o, _ in rank if 'Hand' not in o][0]\n",
    "        \n",
    "def _make_raw_spatial_mats(D, spatial_data, feat_lst=[],\n",
    "                           use_objs=[], use_most_moving=True, window_size=100,\n",
    "                          logdir=None):\n",
    "    \n",
    "    all_objs = [l.strip() for l in open('obj_names.txt').readlines()]\n",
    "    all_feats = [\"relPosX\", \"relPosY\", \"relPosZ\", \"posX\", \"posY\", \"posZ\",\n",
    "                'velX', 'velY', 'velZ', 'relVelX', 'relVelY', 'relVelZ']\n",
    "    \n",
    "    N = len(D)\n",
    "    feats = []\n",
    "    lbls = []\n",
    "    meta = []\n",
    "    if logdir is not None:\n",
    "        logfile = open('%s/feature_dump.tsv'%logdir, 'w')\n",
    "    for i, d in enumerate(D):\n",
    "        sidx = int(d['step'])\n",
    "        start = sidx-window_size\n",
    "        end = sidx+window_size\n",
    "        L = 1+end-start\n",
    "        w = d['lemma']+'_'+d['pos']\n",
    "        p = d['participant']\n",
    "        t = d[\"task\"]\n",
    "        lbls.append(w)\n",
    "        meta.append(p + ' ' + t + ' %s '%sidx + '%s-%s'%(start, end))\n",
    "        fv = []\n",
    "        nv = []\n",
    "        obj_spans = {o: np.zeros((L, len(all_feats))) for o in all_objs}\n",
    "        for si, step in enumerate(range(start, end+1)):\n",
    "            if step in spatial_data[(p, t)]:\n",
    "                sdata = spatial_data[(p, t)][step]\n",
    "                for obj in all_objs:\n",
    "                    if obj in sdata:\n",
    "                        frame = sdata[obj]\n",
    "                        row = [frame[e] for e in all_feats]\n",
    "                        if not(np.isnan(row).any()):\n",
    "                            obj_spans[obj][si, :] = row\n",
    "                            \n",
    "        for obj in use_objs:\n",
    "            x, names = get_features(obj_spans[obj], all_feats, obj_spans, use=feat_lst)\n",
    "            fv += x\n",
    "            nv += [(\"%s_\"%obj)+n for n in names]\n",
    "            \n",
    "        if use_most_moving:\n",
    "            most_moving_obj = get_most_moving(obj_spans, all_feats)\n",
    "            x, names = get_features(obj_spans[most_moving_obj], all_feats, obj_spans, use=feat_lst)\n",
    "            fv += x\n",
    "            nv += [\"most_moving_\"+n for n in names]\n",
    "            fv.append(all_objs.index(most_moving_obj))\n",
    "            nv.append(\"most_moving_obj\")\n",
    "            \n",
    "        for obj in all_objs:\n",
    "            x, names = get_features(obj_spans[obj], all_feats,\n",
    "                                    obj_spans, use=['dist_to_rhand', 'dist_to_lhand', 'avg_vel'])\n",
    "            fv += x\n",
    "            nv += [\"obj_\"+n for n in names]\n",
    "            \n",
    "        if np.isnan(fv).any():\n",
    "            print(\"NaNs!!1\", i, fv)\n",
    "        feats.append(fv)\n",
    "        if logdir is not None:\n",
    "            fdump = {n: f for (n, f) in zip(nv, fv)}\n",
    "            logfile.write('%s\\t%s\\t%s\\t%s-%s\\t%s\\tfdim=%d\\t%s\\n'%(w, p, t, start, end, most_moving_obj,\n",
    "                                                              len(fdump.keys()), json.dumps(fdump)))\n",
    "    if logdir is not None:\n",
    "        logfile.close()\n",
    "    return np.array(feats), lbls, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sims_avged(m1, m2, words, outfile, save=False):\n",
    "    sims = cosine_similarity(m1, m2)\n",
    "    ranks = []\n",
    "    rand = []\n",
    "    lines = []\n",
    "    N = len(words)\n",
    "    for i, w in enumerate(words):\n",
    "        v1 = m1[i, :]\n",
    "        v2 = m2[i, :]\n",
    "        ssim = sims[i, i]\n",
    "        \n",
    "        max_idxs = [e for e in reversed(sims[i, :].argsort())] # to get descending order\n",
    "        max_idx = max_idxs[0]\n",
    "        \n",
    "        # benefit-of-the-doubt MRR computation\n",
    "        eq_set = np.where(sims[i, :] == ssim)[0] # all the words that have the same (max) similarity to query\n",
    "        min_rank = N+1\n",
    "        for idx in eq_set:\n",
    "            min_rank = min(min_rank, np.where(max_idxs == idx)[0])\n",
    "        ranks.append(1/(min_rank+1))\n",
    "        rand.append(1/(random.choice(max_idxs)+1))\n",
    "        lines.append((ssim, '%s\\tselfsim=%.02f\\trank=%d\\tmaxsim=%s (%.02f)'%(w,\n",
    "              ssim, min_rank, words[max_idx], sims[i, max_idx])))\n",
    "    sum_line = \"MRR = %.02f (Ceil=%.02f, Rand=%.02f, Floor=%.02f)\"%(sum(ranks)/len(ranks),\n",
    "                                                                    1, sum(rand)/len(rand), 1/N)\n",
    "    if save:\n",
    "        out = open(outfile, mode='w')\n",
    "        out.write(\"%s\\n\"%(sum_line))\n",
    "        for _, l in sorted(lines, key=lambda e:e[0], reverse=True):\n",
    "            out.write(l+'\\n')\n",
    "        out.close()\n",
    "    return sum_line\n",
    "\n",
    "def get_sims(tr, dv, tr_lbl, dv_lbl, tr_meta, dv_meta, outdir, save=False):\n",
    "    sims = cosine_similarity(dv, tr)\n",
    "    rand = []\n",
    "    debug_lines = []\n",
    "    N = len(dv_lbl)\n",
    "    by_lbl = {}\n",
    "    by_w = {w: [] for w in set(dv_lbl)}\n",
    "    pat3 = {w: [] for w in set(dv_lbl)}\n",
    "    \n",
    "    rand = []\n",
    "    rand_p3 = []\n",
    "    for i, w in enumerate(dv_lbl):\n",
    "\n",
    "        max_idxs = [e for e in reversed(sims[i, :].argsort())] # to get descending order\n",
    "        max_idx = max_idxs[0]\n",
    "        \n",
    "        # find the rank of the most-similar instance of the true word\n",
    "        matches = [idx for idx, u in enumerate(tr_lbl) if u == w]\n",
    "        ranked_correct = [(idx, max_idxs.index(idx)) for idx in matches]\n",
    "        top_correct_idx, top_correct_rank = sorted(ranked_correct, key=lambda e:e[1])[0]\n",
    "        \n",
    "        by_w[w].append(1./(1+top_correct_rank))\n",
    "        pat3[w].append(1 if top_correct_rank < 3 else 0)\n",
    "        \n",
    "        cells = {}\n",
    "        cells['word'] = w\n",
    "        p, t, step, steps = dv_meta[i].split()\n",
    "        cells['participant'] = p\n",
    "        cells['task'] = t\n",
    "        cells['steps'] = steps\n",
    "        cells['correct'] = (tr_lbl[max_idx] == w)\n",
    "        cells['pred'] = (tr_lbl[max_idx])\n",
    "        cells['rank_w'] = top_correct_rank\n",
    "        cells['sim_w'] = '%.06f'%sims[i, top_correct_idx]\n",
    "        cells['sim_pred'] = '%.06f'%sims[i, max_idx]\n",
    "        debug_lines.append((w, cells))\n",
    "        \n",
    "    \n",
    "        # bootstrap CI for random baseline\n",
    "        #for _ in range(100):\n",
    "        order = max_idxs\n",
    "        random.shuffle(order)\n",
    "        rank = order.index(matches[0])\n",
    "        rand.append(1./(rank+1))\n",
    "        rand_p3.append(1 if rank < 3 else 0)\n",
    "                \n",
    "    rand = sorted(rand)\n",
    "    out = open('%s/summary.txt'%outdir, 'w')\n",
    "    out.write(\"Worst-Case Baseline:\\t%.02f\\n\"%(1/len(tr_lbl)))\n",
    "    out.write(\"Random Baseline:\\t%.02f (%.02f -- %.02f)\\t%.02f\\n\"%(sum(rand)/len(rand),\n",
    "                                                                   rand[0], rand[-1], sum(rand_p3)/len(rand_p3)))\n",
    "    print(\"Worst-Case Baseline:\\t%.02f\"%(1/len(tr_lbl)))\n",
    "    print(\"Random Baseline:\\t%.02f (%.02f -- %.02f)\\t%.02f\"%(sum(rand)/len(rand),\n",
    "                                                                   rand[0], rand[-1], sum(rand_p3)/len(rand_p3)))    \n",
    "    lines = []\n",
    "    macro = []\n",
    "    p3s = []\n",
    "    for w, lst in sorted(by_w.items()):\n",
    "        mrr = sum(lst)/len(lst)\n",
    "        p3 = sum(pat3[w])/len(pat3[w])\n",
    "        macro.append(mrr)\n",
    "        p3s.append(p3)\n",
    "        lines.append('%s\\t%.02f\\t%.02f\\t%d'%(w, mrr, p3, len(lst)))\n",
    "        \n",
    "    out.write(\"Macro Avg. MRR:\\t%.02f\\t%.02f\\n\"%(sum(macro)/len(macro), sum(p3s)/len(p3s)))\n",
    "    print(\"Macro Avg. MRR:\\t%.02f\\t%.02f\"%(sum(macro)/len(macro), sum(p3s)/len(p3s)))\n",
    "\n",
    "    for line in lines:\n",
    "        out.write(line+'\\n')\n",
    "    out.close()\n",
    "    \n",
    "    writer = csv.DictWriter(open(\"%s/detail.tsv\"%outdir, 'w'),\n",
    "                            fieldnames=['word','correct','pred','rank_w','sim_w','sim_pred',\n",
    "                                       'participant', 'task', 'steps'], \n",
    "                            delimiter='\\t')\n",
    "    writer.writeheader()\n",
    "    for w, line in sorted(debug_lines, key=lambda e:e[0]):\n",
    "        writer.writerow(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for Model Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0 3_1b task1\n",
      "500000 4_2b task3\n",
      "1000000 8_2a task3\n",
      "1500000 1_1a task5\n",
      "2000000 16_2b task4\n",
      "2500000 13_1a task5\n",
      "3000000 14_2a task1\n"
     ]
    }
   ],
   "source": [
    "# this takes a couple minutes\n",
    "spatial_data = load_all_spatial_data(\"all_spatial_data_all_objs_relabspos_alignedsteps.txt\",\n",
    "                                     fnames = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'aligned_data/may/partial_states_kappa=10.txt'\n",
    "config = json.loads(open('config.json').read())\n",
    "\n",
    "config['MODE'] = 'raw'\n",
    "config['LOG'] = True\n",
    "config['DATA'] = data_file\n",
    "config['K'] = 5\n",
    "config['WINDOW'] = int(round(90 * 5)) # 90 = 1 second\n",
    "config['FEATURES'] = [\n",
    "                    'rel_pos_se',\n",
    "                    'rel_st_mx', \n",
    "                    'rel_end_mx', \n",
    "                    'rel_pos_var',\n",
    "                    'rel_pos_avg',\n",
    "    \n",
    "                    'abs_st_mx',\n",
    "                    'abs_end_mx',\n",
    "                    'abs_pos_se',\n",
    "    \n",
    "                    'rel_vel_avg',\n",
    "                    'vel_avg',\n",
    "    \n",
    "                    'dist_to_rhand',\n",
    "                    'dist_to_lhand',\n",
    "                ]\n",
    "config['MOST_MOVING'] = True\n",
    "config['OBJS'] = ['RightHand', 'LeftHand', 'Head']\n",
    "#config['OBJS'] = ['RightHand', 'Apple', 'Ball', 'Banana', 'Bear', 'Book', 'Bowl', 'Cup',\n",
    "#                'Dinosaur', 'Doll', 'Fork', 'Head', 'Knife', 'Lamp', 'LeftHand',\n",
    "#                'Plant', 'Spoon', 'Toy']\n",
    "\n",
    "inp = config['MODE'].split('.')[0].split('/')[-1]\n",
    "inp += '_' + name_from_config(config)\n",
    "rootdir = 'spatial_eval/%s/'%inp\n",
    "if not os.path.exists(rootdir):\n",
    "    os.makedirs(rootdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data: 18826 items\n",
      "Action Vocab=3181; Word Vocab=1324\n",
      "Filtered Data: 968 items\n",
      "X: 968 x 343\n",
      "y: 1 x 968\n"
     ]
    }
   ],
   "source": [
    "vocab_lsts, count_lsts, top_words_candidates, X, y, meta = prep_data(config, logdir=rootdir)\n",
    "\n",
    "sys.stderr.write(\"X: %s x %s\\n\"%X.shape)\n",
    "sys.stderr.write(\"y: 1 x %s\\n\"%len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (358, 343)\n",
      "X_dev: (27, 343)\n",
      "y_train: 358\n",
      "y_dev: 27\n"
     ]
    }
   ],
   "source": [
    "X_filt, y_filt, m_filt = filter_and_dedup(X, y, meta, top_words_candidates)\n",
    "train_tup, dev_tup, test_tup = train_test_split(X_filt, y_filt, m_filt, config['TEST'], config['DEV'])\n",
    "\n",
    "#dev_tup = train_tup\n",
    "\n",
    "X_train, y_train, m_train = train_tup\n",
    "X_dev, y_dev, m_dev = dev_tup\n",
    "top_words = list(set(y_filt))\n",
    "\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"X_dev:\", X_dev.shape)\n",
    "print(\"y_train:\", len(y_train))\n",
    "print(\"y_dev:\", len(y_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_red: (358, 14)\n",
      "X_dev_red: (27, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of classes = 15\n",
      "Training size = 358 x 343\n",
      "Dim to reduce to = 50.000\n",
      "/Users/ellie/miniconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning:\n",
      "\n",
      "The priors do not sum to 1. Renormalizing\n",
      "\n",
      "/Users/ellie/miniconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning:\n",
      "\n",
      "Variables are collinear.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config['RED'] = True\n",
    "config['DIM'] = 50\n",
    "config['SUP'] = True\n",
    "\n",
    "if config['RED'] == True:\n",
    "    trained_reducer = reduce_mats(\n",
    "                train_tup, dev_tup, test_tup,\n",
    "                dim=config['DIM'], supervised=config['SUP'])\n",
    "    X_train = trained_reducer.transform(X_train)\n",
    "    X_dev = trained_reducer.transform(X_dev)\n",
    "    train_tup = (X_train, train_tup[1], train_tup[2])\n",
    "    dev_tup = (X_dev, dev_tup[1], dev_tup[2])\n",
    "    print(\"X_train_red:\", X_train.shape)\n",
    "    print(\"X_dev_red:\", X_dev.shape)\n",
    "    \n",
    "repdir, figdir = make_directories(config)\n",
    "json.dump(config, open(repdir+'/config.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worst-Case Baseline:\t0.07\n",
      "Random Baseline:\t0.20 (0.07 -- 1.00)\t0.15\n",
      "Macro Avg. MRR:\t0.38\t0.38\n"
     ]
    }
   ],
   "source": [
    "mn = X_train.min()\n",
    "mx = X_train.max()\n",
    "avg_train = avg_mat(train_tup, top_words)\n",
    "avg_dev = avg_mat(dev_tup, top_words)\n",
    "\n",
    "sum_lines = []\n",
    "get_sims(avg_train, X_dev, top_words, y_dev, None, m_dev, repdir, save=config['LOG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data: 107 items\n"
     ]
    }
   ],
   "source": [
    "blind_X, blind_y, blind_meta = eval_on_random(config, spatial_data, vocab_lsts, count_lsts)\n",
    "if config['RED'] == True:\n",
    "    blind_X = trained_reducer.transform(blind_X)\n",
    "    \n",
    "outfile = open('%s/blind_eval.txt'%repdir, 'w')\n",
    "\n",
    "sims = cosine_similarity(blind_X, avg_train)\n",
    "for i in range(len(blind_meta)):\n",
    "    \n",
    "    max_idxs = [e for e in reversed(sims[i, :].argsort())] # to get descending order\n",
    "    plst = []\n",
    "    for idx in max_idxs:\n",
    "        plst.append('%s=%.04f'%(top_words[idx], sims[i, idx]))\n",
    "    \n",
    "    outfile.write('%s\\t%s\\t%s\\n'%(blind_meta[i], blind_y[i], '\\t'.join(plst)))\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reducer = TruncatedSVD(n_components=2)\n",
    "reducer = TSNE(n_components=2, perplexity=10)\n",
    "\n",
    "#red_tr = reducer.fit_transform(X_train_red)\n",
    "#red_dv = reducer.fit_transform(X_dev_red)\n",
    "print(X_train.shape)\n",
    "print(X_dev.shape)\n",
    "print(avg_train.shape)\n",
    "print(avg_dev.shape)\n",
    "red = reducer.fit_transform(np.vstack((X_train, X_dev, avg_train, avg_dev)))\n",
    "#red = reducer.fit_transform(np.vstack((X_train, X_dev)))\n",
    "#red = np.vstack((X_train, X_dev))\n",
    "print(red.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "word_order = sorted(list(set(y_train)))\n",
    "part_order = sorted(list(set([e.split()[0] for e in meta])))\n",
    "print(word_order)\n",
    "print(part_order)\n",
    "train_end = len(y_train)\n",
    "dev_end = train_end + len(y_dev)\n",
    "avg_train_end = dev_end + len(top_words)\n",
    "avg_dev_end = avg_train_end + len(top_words)\n",
    "\n",
    "color_by_word = True\n",
    "\n",
    "L = len(color_lst)\n",
    "\n",
    "# train words\n",
    "if color_by_word:\n",
    "    tr_colors = [color_lst[word_order.index(y)%L] for y in y_train]\n",
    "    dv_colors = [color_lst[word_order.index(y)%L] for y in y_dev]\n",
    "else:\n",
    "    tr_colors = [color_lst[part_order.index(p.split()[0])%L] for p in m_train]\n",
    "    dv_colors = [color_lst[part_order.index(p.split()[0])%L] for p in m_dev]\n",
    "    \n",
    "plt.scatter(red[:train_end, 0], red[:train_end, 1], s=10, color=tr_colors)\n",
    "\n",
    "# centroids of train words\n",
    "if color_by_word:\n",
    "    plt.scatter(red[dev_end:avg_train_end, 0], red[dev_end:avg_train_end, 1], s=100,\n",
    "            color=[color_lst[word_order.index(y)%L] for y in top_words], linewidth=1, edgecolor='k')\n",
    "\n",
    "# dev words\n",
    "plt.scatter(red[train_end:dev_end, 0], red[train_end:dev_end, 1], s=30, color=dv_colors,\n",
    "            marker='s', linewidth=1, edgecolor='k')\n",
    "\n",
    "# centroids of dev words\n",
    "#plt.scatter(red[avg_train_end:, 0], red[avg_train_end:, 1], s=50,\n",
    "#            color=[color_lst[word_order.index(y)%L] for y in top_words], marker='s', linewidth=1, edgecolor='k')\n",
    "\n",
    "if color_by_word:\n",
    "    legend_elements = [Patch(facecolor=color_lst[i%L], label=word_order[i]) for i in range(len(word_order))]\n",
    "else:\n",
    "    legend_elements = [Patch(facecolor=color_lst[i%L], label=part_order[i]) for i in range(len(part_order))]\n",
    "plt.legend(handles=legend_elements, loc='lower center', ncol=5, bbox_to_anchor=(0.5, -0.2))\n",
    "\n",
    "plt.savefig(\"%s/big_scatter.pdf\"%figdir, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_mat_figures(outpath, 'raw_train', avg_train, top_words, mn, mx, 'png', save=config['LOG'])\n",
    "vocab_mat_figures(outpath, 'raw_dev', avg_dev, top_words, mn, mx, 'png', save=config['LOG'])\n",
    "vocab_mat_figures_indv(outpath, train_tup, mn, mx, 'png', save=config['LOG'])\n",
    "vocab_mat_figures_indv(outpath, dev_tup, mn, mx, 'png', save=config['LOG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    print(vocab[i])\n",
    "    plt.hist([X_train[[i for i,y in enumerate(y_train) if (y == 'pick_VERB') ], i],\n",
    "         X_train[[i for i,y in enumerate(y_train) if (y == 'put_VERB') ], i]], normed=True)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
